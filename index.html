<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252"><script id="twitter-wjs" src="./index_files/widgets.js"></script><script async="" src="./index_files/analytics.js"></script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-57467968-1', 'auto');
  ga('send', 'pageview');

</script>





<link href="./index_files/style.css" rel="stylesheet">
<title>Website of Michael Gygli</title>
<script type="text/javascript" charset="utf-8" async="" src="./index_files/timeline.e7653a8bc8be5342f5ecf22ae2e65c92.js"></script></head>

<body>

		<div id="main">
		
				<div id="top_nav">
					<div class="top_link"><a href="publications.html">Publications</a></div>
		  			<div class="top_link"><a href="contact.html">Contact</a></div>
					<div class="top_link_right"><a href="index.html">Michael Gygli, PhD</a></div>
				</div>

			<div class="clear"></div>
			
			
			<div id="content_handler">
				<div class="line">Michael Gygli, Research Scientist at Google</div>

<p>Hi and welcome. My name is Michael Gygli and I am a Research Scientist at Google Zurich in the group of Prof. Vittorio Ferrari.
<p>Previously I was the Head of AI at <a href="https://gifs.com/">gifs.com</a>, where I led the Machine Learning and Computer Vision efforts aimed at making video editing and gif creation intelligent and easy. Before that, I was a PhD student under the supervision of Prof. Luc Van Gool at the <a href="http://www.vision.ee.ethz.ch/">Computer Vision Laboratory of ETH Zurich</a>.

The research during my PhD was focused on video summarization, but I am also interested in deep learning, affective attributes, submodularity and data summarization in general.</p>
<p>Check the <a href="publications.html">publications page</a> or my <a href="http://scholar.google.ch/citations?user=xpyADpwAAAAJ">Google Scholar profile</a> for previous publications. I also provide code for some of my work on <a href="https://github.com/gyglim">Github</a>.</p>
<p>Slides for my CVPR tutorial talk "Video Summarization as Subset Selection": <a href="https://t.co/mQIpxMab3v">Click here</a>
</p><div class="cont_box1">
<img src="./index_files/michael_gygli.jpg" width="200">
<br>
<p>That's me (was, a few years ago)</p>

</div>

<a class="twitter-timeline" data-height="450" data-width="600" href="https://twitter.com/GygliMichael?ref_src=twsrc%5Etfw">Tweets by Michael Gygli</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div>

					<div class="clear"></div>
				<div class="line">Highlighted News</div>	

 
<p>
<table cellspacing="4">
<tbody><tr><td width="120px">
<tr><td width="100px">
2018 February: </td><td>I joined Google as a Research Scientist.</td></tr>
<tr><td width="100px">
2017 December: </td><td>We have released a new <a href="https://github.com/gyglim/personalized-highlights-dataset">dataset for personalized highlight detection</a>
with data from more than 13000 users.</td></tr>
<tr><td width="100px">
2017 July: </td><td>The PathTrack paper was accepted to ICCV and the query-adaptive summarization paper to ACM Multimedia.</td></tr>

<tr><td width="100px">
2017 June: </td><td>Video and slides of my presentation "CNNs in Video analysis - An overview, biased to fast methods" at the <a href="https://www.meetup.com/de-DE/Zurich-Machine-Learning/">ML meetup Zurich</a> <a href="https://www.youtube.com/watch?v=xRLeLQV8kL8">are now online</a>.</td></tr>
<tr><td width="100px">
2017 June: </td><td>Code for our ICML paper on Deep Value Networks is now available on <a href="https://github.com/gyglim/dvn">github</a>.</td></tr>
<tr><td width="100px">
2017 May: </td><td>Our pre-print <a href="https://arxiv.org/abs/1705.08214">Ridiculously Fast Shot Boundary Detection with Fully Convolutional Neural Networks</a> is now on arXiv.</td></tr>

<!--
<tr><td width='100px'>
2017 Mar: </td><td><b>I am co-organizing the  <a href='http://www.multimediaeval.org/mediaeval2017/mediainterestingness/'>Predicting Media Interestingness Task</a> at MediaEval.</b>
The goal of the challenge is to find interesting video content. We are welcoming submissions.</td></tr>

<tr><td width='100px'>
2017 Mar: </td><td>Our pre-print <a href="https://t.co/hRQkiWQGp4">Deep Value Networks Learn to Evaluate and Iteratively Refine Structured Outputs</a> is now on arXiv.</td></tr>

<tr><td width='100px'>
2017 Mar: </td><td>Our pre-print <a href="https://arxiv.org/pdf/1703.02437v1.pdf">PathTrack: Fast Trajectory Annotation with Path Supervision</a> is now on arXiv.</td></tr>
<tr><td width='100px'>
2017 Jan: </td><td>Our work <a href="https://arxiv.org/pdf/1701.00599">AENet: Learning Deep Audio Features for Video Analysis</a> is available on arXiv.
We provide the pre-trained network and code to run it on <a href='https://github.com/znaoya/aenet'>GitHub</a>.</td></tr>
-->
<tr><td width="100px">
2017 Jan: </td><td>I joined <a href="https://gifs.com/">gifs.com</a></td></tr>

<!--
<tr><td width='100px'>
2017 Jan: </td><td>I joined <a href="https://gifs.com">gifs.com</a> as a Computer Vision engineer. We are hiring - <a href=mailto:jobs@gifyt.com?subject='Here%20is%20why%20I%20should%20work%20in%20ML/CV%20at%20gifs'">get in touch</a> if you are interested.</td></tr>

<tr><td width="100px">
2016 Aug-Nov: </td><td>I was interning at <a href="https://research.google.com/teams/brain/">Google Brain</a>.</td></tr>

<tr><td width='100px'>
2016 Jun: </td><td>I co-organized the tutorial "Optimization Algorithms for Subset Selection and Summarization in Large Data Sets" at CVPR 2016. You can find my slides on Video Summarization as Subset Selection <a href="https://t.co/mQIpxMab3v">here</a>.</td></tr>
<tr><td width='100px'>

2016 Jun: </td><td>We now provide the pretrained model and some demo code for the Video2GIF paper on  <a href='https://github.com/gyglim/video2gif_code'>GitHub</a>.</td></tr>
<tr><td width='100px'>

2016 Jun: </td><td>We now have a <a href='http://people.ee.ethz.ch/~gyglim/work_public/autogif/'>demo for the Video2GIF paper</a>.</td></tr>
<tr><td>

<tr><td width='100px'>
2016 Jul: </td><td>Our short paper on GIF interestingness is accepted to ACM MM 2016.</td></tr>
2016 Jun: </td><td>Our paper on audio event detection with CNNs got accepted to InterSpeech 2016.</td></tr>
<tr><td width='100px'>

2016 Apr: </td><td>The Video2GIF dataset (100k GIFs and their source videos) is now available on <a href='https://github.com/gyglim/video2gif_dataset'>Github</a>.</td></tr>
2016 Mar: </td><td>Two papers accepted to CVPR: "Video2GIF: Automatic Generation of Animated GIFs from Video" and "Predicting When Saliency Maps are Accurate and Eye Fixations Consistent"</td></tr>
<tr><td width='100px'>

<tr><td>
<tr><td>
2015 Jun: </td><td>We now provide  <a href='https://github.com/gyglim/gm_submodular'><b>Python code</b> for submodular structured learning</a>.</td></tr>

<tr><td>
2014 Summer: </td><td>Attended the <a href='http://rnls.ethz.ch/summerschool2014/'>Summer School of the Research Network on Learning Systems</a> at ETH Zurch, Switzerland and
the <a href='http://svg.dmi.unict.it/icvss2014/'>International Computer Vision Summer School</a> in Sicily, Italy.
</td></tr><tr><td>
2013 Aug: </td><td>Our paper <a href='interest/iccv13.php'>The Interestingness of Images</a> got accepted to <a href="http://www.iccv2013.org/">ICCV 2013</a> in Sydney, Australia
</td></tr>
<tr><td>
2013 Mar: </td><td>Our paper <a href="publications.php">"Sparse Quantization for Patch Description"</a> got accepted to <a href="http://www.pamitc.org/cvpr13/">CVPR 2013</a>
</td></tr>
<tr><td>
2012 Dec: </td><td>Joined the <a href="http://varcity.eu/">Varcity project</a> as a PhD student.</td></tr>
-->
</tbody></table>
</p>
				<div class="clear"></div>
<br>
				<div class="line">Recent Papers</div>	
<p>
<table cellspacing="2">

<tbody>
<tr><td width="120px">arXiv 2018</td><td><p><b><a href="https://arxiv.org/abs/1804.06604">PHD-GIFs: Personalized Highlight Detection for Automatic GIF Creation</a></b><br>A. Garcia del Molino, <span style="font-size:16px; font-style:italic;">M. Gygli</span></p></td></tr>

<tr><td width="120px">arXiv 2017</td><td><p><b><a href="https://arxiv.org/abs/1801.00269">Interactive Video Object Segmentation in the Wild</a></b><br>A.Benard, <span style="font-size:16px; font-style:italic;">M. Gygli</span></p></td></tr>

<tr><td width="120px">ICML 2017</td><td><p><b><a href="papers/dvn_imcl17.pdf">Deep Value Networks Learn to Evaluate and Iteratively Refine Structured Outputs</a></b><br><span style="font-size:16px; font-style:italic;">M. Gygli</span>,
M. Norouzi, A. Angelova</p></td></tr>

<tr><td width="120px">arXiv 2017</td><td><p><b><a href="https://arxiv.org/abs/1705.08214">Ridiculously Fast Shot Boundary Detection with Fully Convolutional Neural Networks</a></b><br><span style="font-size:16px; font-style:italic;">M. Gygli</span></p></td></tr>

<tr><td width="120px">Transactions on Multimedia 2017</td><td><p><b><a href="papers/aenet-tmm17.pdf">AENet: Learning Deep Audio Features for Video
Analysis</a></b><br>N. Takahashi, <span style="font-size:16px; font-style:italic;">M. Gygli</span>, L. Van Gool</p></td></tr>

<tr><td width="120px">ICCV 2017</td><td><p><b><a href="papers/pathtrack_iccv17.pdf">PathTrack: Fast Trajectory Annotation with Path Supervision</a></b><br>S. Manen, <span style="font-size:16px; font-style:italic;">M. Gygli</span>,
D. Dai, L. Van Gool</p></td></tr>

<tr><td width="120px">ACM MM 2017</td><td><p><b><a href="papers/query_adpative_summaries_acm_mm17.pdf">Query-adaptive Video Summarization via Quality-aware Relevance Estimation</a></b><br>A. Vasudevan, <span style="font-size:16px; font-style:italic;">M. Gygli</span>,
A. Volokitin, L. Van Gool</p></td></tr>

<!--
<tr><td width='120px'>ACM MM 2016</td><td><p><b><a href='https://t.co/hRQkiWQGp4'>Analyzing and Predicting GIF Interestingness</a></b></br><span style="font-size:16px; font-style:italic;">M. Gygli</span>,
M. Soleymani (equal contribution)</td></tr>


<tr><td width='120px'>InterSpeech 2016</td><td><p><b><a href='http://arxiv.org/pdf/1604.07160.pdf'>Deep Convolutional Neural Networks and Data Augmentation for Acoustic
Event Detection</a></b></br>N. Takahashi <span style="font-size:16px; font-style:italic;">M. Gygli</span>,
B. Pfister, L. Van Gool</td></tr>

<tr><td width='120px'>CVPR 2016</td><td><p><b><a href='video2gif_cvp16.pdf'>Video2GIF: Automatic Generation of Animated GIFs from Video</a></b></br> <span style="font-size:16px; font-style:italic;">M. Gygli</span>,
Y. Song, L. Cao</td></tr>

-->
</tbody></table>
			</div>
			<div class="clear"></div>



</div><iframe scrolling="no" frameborder="0" allowtransparency="true" src="./index_files/widget_iframe.31849fd556d065e6364d2ceb2dcd1e60.html" title="Twitter settings iframe" style="display: none;"></iframe><iframe id="rufous-sandbox" scrolling="no" frameborder="0" allowtransparency="true" allowfullscreen="true" style="position: absolute; visibility: hidden; display: none; width: 0px; height: 0px; padding: 0px; border: none;" title="Twitter analytics iframe" src="./index_files/saved_resource(1).html"></iframe></body></html>
